# 强化学习入门指南：从零开始使用 Gymnasium

本指南旨在为初学者提供一个全面且详细的强化学习（Reinforcement Learning, RL）入门路径，并结合 [Gymnasium](https://gymnasium.farama.org/) 库进行实践教学。

## 1. 什么是强化学习？

强化学习是机器学习的一个分支，关注于如何让智能体（Agent）在一个环境（Environment）中通过与环境的交互学习最优行为策略，以最大化累积奖励（Cumulative Reward）。

核心概念：
*   **智能体 (Agent)**: 学习并做出决策的实体。
*   **环境 (Environment)**: 智能体所处的外部世界，接收智能体的动作并返回新的状态和奖励。
*   **状态 (State)**: 对环境的描述，智能体基于状态做出决策。
*   **动作 (Action)**: 智能体在给定状态下可以执行的操作。
*   **奖励 (Reward)**: 环境对智能体动作的反馈信号，通常是一个标量值，表示动作的好坏。智能体的目标是最大化长期累积奖励。
*   **策略 (Policy)**: 智能体从状态到动作的映射，定义了智能体在特定状态下采取何种动作。
*   **价值函数 (Value Function)**: 评估给定状态或状态-动作对的长期回报。

强化学习的流程可以概括为：
1.  智能体观察环境，得到当前状态 `s`。
2.  智能体根据策略选择一个动作 `a`。
3.  智能体执行动作 `a`，环境进入新状态 `s'` 并返回奖励 `r`。
4.  智能体利用 `(s, a, s', r)` 更新其策略或价值函数。
5.  重复以上过程，直到达到某个终止条件。

## 2. Gymnasium 库简介

[Gymnasium](https://gymnasium.farama.org/) 是一个用于开发和比较强化学习算法的工具包，它提供了一系列标准化的环境（Environments），使得研究人员和开发者可以专注于算法的设计而非环境的搭建。

### 2.1 安装 Gymnasium

在您开始之前，请确保您已经安装了 Python (建议 3.8+)。在 [`gymnasium_learning`](gymnasium_learning/) 目录下，您可以通过 `requirements.txt` 文件安装必要的库：

```bash
cd gymnasium_learning
pip install -r requirements.txt
```

如果 `requirements.txt` 不存在或您想手动安装 `gymnasium`，可以使用：

```bash
pip install gymnasium
```

为了运行一些经典控制环境（如 CartPole），您可能还需要安装 `pygame`：

```bash
pip install gymnasium[classic-control] pygame
```

### 2.2 Gymnasium 基本使用

所有 Gymnasium 环境都遵循一个统一的 API 接口，这使得切换环境和测试不同算法变得非常方便。

#### 创建环境

使用 `gymnasium.make()` 函数创建环境实例：

```python
import gymnasium as gym

# 创建一个 CartPole 环境
env = gym.make("CartPole-v1")
```

#### 环境交互

环境交互主要涉及以下几个方法：

*   `observation_space`: 定义了观测值的结构和范围。
*   `action_space`: 定义了智能体可以采取的动作的结构和范围。
*   `reset()`: 重置环境到初始状态，返回初始观测和信息。
*   `step(action)`: 智能体执行一个动作，环境返回新观测、奖励、是否终止、是否截断和额外信息。
    *   `observation (object)`: 环境的观测，表示环境的当前状态。
    *   `reward (float)`: 上一个动作获得的奖励。
    *   `terminated (bool)`: 布尔值，表示智能体是否达到终止状态（例如，游戏结束）。
    *   `truncated (bool)`: 布尔值，表示智能体是否因时间限制等原因被截断（例如，回合达到最大步数）。
    *   `info (dict)`: 包含诊断信息的字典，用于调试和学习，例如概率等。

*   `render()`: 渲染环境的当前状态（可视化）。
*   `close()`: 关闭环境，释放资源。

一个简单的环境交互示例：

```python
import gymnasium as gym

env = gym.make("CartPole-v1", render_mode="human") # 设置 render_mode="human" 来可视化环境

observation, info = env.reset() # 重置环境，获取初始观测和信息

for _ in range(100): # 模拟 100 步
    action = env.action_space.sample() # 从动作空间中随机选择一个动作
    observation, reward, terminated, truncated, info = env.step(action) # 执行动作
    env.render() # 渲染环境

    if terminated or truncated:
        observation, info = env.reset() # 如果回合结束，重置环境

env.close() # 关闭环境
```

#### 动作空间和观测空间

每个环境都有 `action_space` 和 `observation_space` 属性，它们定义了智能体可以采取的动作类型和环境观测值的类型。

*   **离散动作空间 (Discrete)**: `gym.spaces.Discrete(n)` 表示有 `n` 个离散动作，动作通常是 `0` 到 `n-1` 的整数。
    *   示例: `env.action_space.n` (动作数量), `env.action_space.sample()` (随机动作)。
*   **连续动作空间 (Box)**: `gym.spaces.Box(low, high, shape, dtype)` 表示连续动作空间，动作是指定范围内的浮点数数组。
    *   示例: `env.action_space.shape` (动作形状), `env.action_space.low`, `env.action_space.high`。

同样，观测空间也可能是离散或连续的。

## 3. 强化学习基础概念

### 3.1 马尔可夫决策过程 (MDP)

强化学习问题通常被建模为马尔可夫决策过程 (MDP)。一个 MDP 由以下五元组定义：

*   `S`: 有限状态集合。
*   `A`: 有限动作集合。
*   `P(s' | s, a)`: 状态转移概率，表示在状态 `s` 执行动作 `a` 后，转移到状态 `s'` 的概率。
*   `R(s, a, s')`: 奖励函数，表示从状态 `s` 执行动作 `a` 转移到状态 `s'` 获得的奖励。
*   `γ (gamma)`: 折扣因子 (Discount Factor)，`0 <= γ <= 1`，用于权衡即时奖励和未来奖励的重要性。

### 3.2 策略 (Policy)

策略 `π(a | s)` 定义了在给定状态 `s` 下采取动作 `a` 的概率。智能体的目标是学习一个最优策略 `π*`，使得从任何状态开始的期望累积奖励最大化。

### 3.3 价值函数 (Value Functions)

价值函数用于评估策略的好坏。

*   **状态价值函数 (State-Value Function)** `V_π(s)`: 在遵循策略 `π` 的前提下，从状态 `s` 开始的期望累积奖励。
*   **动作价值函数 (Action-Value Function)** `Q_π(s, a)`: 在遵循策略 `π` 的前提下，在状态 `s` 执行动作 `a` 后，再继续遵循策略 `π` 的期望累积奖励。

**贝尔曼方程 (Bellman Equation)** 是强化学习中的核心方程，它描述了价值函数之间的递归关系。

*   **贝尔曼期望方程 (Bellman Expectation Equation)**：
    `V_π(s) = Σ_a π(a|s) [ R(s, a) + γ Σ_s' P(s'|s, a) V_π(s') ]`
    `Q_π(s, a) = R(s, a) + γ Σ_s' P(s'|s, a) V_π(s')`

*   **贝尔曼最优方程 (Bellman Optimality Equation)**：
    `V*(s) = max_a [ R(s, a) + γ Σ_s' P(s'|s, a) V*(s') ]`
    `Q*(s, a) = R(s, a) + γ Σ_s' P(s'|s, a) max_a' Q*(s', a')`

学习这些价值函数是许多强化学习算法（如 Q-Learning, SARSA）的基础。

## 4. Q-Learning 算法

Q-Learning 是一种基于价值的无模型（model-free）强化学习算法，用于学习最优动作价值函数 `Q*(s, a)`。它不需要知道环境的状态转移概率 `P(s' | s, a)` 和奖励函数 `R(s, a, s')`。

### 4.1 Q-Table

Q-Learning 的核心是一个 Q 表（Q-Table），它是一个二维数组，存储了每个状态-动作对的 Q 值。

`Q[state, action]`

### 4.2 Q-Value 更新规则

Q-Learning 通过以下更新规则迭代地更新 Q 值：

`Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') - Q(s, a)]`

其中：
*   `s`: 当前状态。
*   `a`: 当前动作。
*   `r`: 执行动作 `a` 后获得的即时奖励。
*   `s'`: 执行动作 `a` 后达到的新状态。
*   `α (alpha)`: 学习率 (Learning Rate)，`0 < α <= 1`，控制每次更新的步长。
*   `γ (gamma)`: 折扣因子 (Discount Factor)，`0 <= γ <= 1`，用于权衡即时奖励和未来奖励的重要性。
*   `max_a' Q(s', a')`: 在新状态 `s'` 下，所有可能动作中最大的 Q 值。这代表了未来可能获得的最大期望奖励。

### 4.3 探索与利用 (Exploration vs. Exploitation)

在 Q-Learning 中，智能体需要平衡探索（尝试新的动作）和利用（选择已知最好的动作）。常用的策略是 **ε-贪婪策略 (ε-greedy policy)**：

*   以 `ε` 的概率随机选择一个动作（探索）。
*   以 `1 - ε` 的概率选择当前 Q 值最大的动作（利用）。

在训练过程中，通常会随着时间衰减 `ε` 的值，从较高的探索率逐渐降低到较低的探索率，以使智能体在学习后期更多地利用已学到的知识。

### 4.4 Q-Learning 示例：CartPole 环境

CartPole 环境的目标是让一个小车上的杆子保持直立。

```python
import gymnasium as gym
import numpy as np

# 1. 创建环境
env = gym.make("CartPole-v1")

# 2. 定义 Q-Table 的维度 (需要对连续状态进行离散化)
# CartPole 的观测空间是连续的，需要手动将其离散化
# 杆子角度 (pole angle) 和杆子角速度 (pole velocity) 是最关键的两个状态
# 简单离散化：将每个连续观测值分成若干个区间

# 定义每个观测维度离散化的桶数量
pos_bins = np.linspace(-2.4, 2.4, 10) # 车的位置
vel_bins = np.linspace(-4.0, 4.0, 10) # 车的速度
angle_bins = np.linspace(-0.2095, 0.2095, 10) # 杆子角度
angle_vel_bins = np.linspace(-4.0, 4.0, 10) # 杆子角速度

# 定义一个函数来将连续观测值映射到离散的桶索引
def discretize_state(observation):
    cart_pos, cart_vel, pole_angle, pole_vel = observation
    cart_pos_idx = np.digitize(cart_pos, pos_bins)
    cart_vel_idx = np.digitize(cart_vel, vel_bins)
    pole_angle_idx = np.digitize(pole_angle, angle_bins)
    pole_vel_idx = np.digitize(pole_vel, angle_vel_bins)
    return (cart_pos_idx, cart_vel_idx, pole_angle_idx, pole_vel_idx)

# Q-Table 的大小取决于离散化后的状态空间和动作空间
state_space_size = (len(pos_bins) + 1, len(vel_bins) + 1, len(angle_bins) + 1, len(angle_vel_bins) + 1)
action_space_size = env.action_space.n # CartPole 有 2 个离散动作: 0 (左), 1 (右)

q_table = np.zeros(state_space_size + (action_space_size,))

# 3. 设置超参数
learning_rate = 0.1       # α
discount_factor = 0.99    # γ
epsilon = 1.0             # 初始探索率 ε
min_epsilon = 0.01        # 最小探索率
epsilon_decay_rate = 0.001 # 探索率衰减率

num_episodes = 2000       # 训练回合数
max_steps_per_episode = 200 # 每个回合的最大步数

# 4. 训练智能体
for episode in range(num_episodes):
    observation, info = env.reset()
    state = discretize_state(observation)
    terminated = False
    truncated = False
    
    for step in range(max_steps_per_episode):
        # ε-贪婪策略：选择动作
        if np.random.uniform(0, 1) < epsilon:
            action = env.action_space.sample() # 探索
        else:
            action = np.argmax(q_table[state]) # 利用

        # 执行动作并获取结果
        new_observation, reward, terminated, truncated, info = env.step(action)
        new_state = discretize_state(new_observation)

        # Q 值更新
        if not terminated and not truncated: # 如果回合未结束
            max_future_q = np.max(q_table[new_state])
            current_q = q_table[state + (action,)]
            new_q = current_q + learning_rate * (reward + discount_factor * max_future_q - current_q)
            q_table[state + (action,)] = new_q
        else: # 如果回合结束 (终止或截断)
            current_q = q_table[state + (action,)]
            new_q = current_q + learning_rate * (reward - current_q) # 终止状态没有未来奖励
            q_table[state + (action,)] = new_q

        state = new_state

        if terminated or truncated:
            break
    
    # 衰减 epsilon
    epsilon = max(min_epsilon, epsilon - epsilon_decay_rate)

    if episode % 100 == 0:
        print(f"Episode: {episode}, Epsilon: {epsilon:.2f}")

print("训练完成！")

# 5. 测试训练好的智能体 (可选)
# env_render = gym.make("CartPole-v1", render_mode="human")
# observation, info = env_render.reset()
# state = discretize_state(observation)
# terminated = False
# truncated = False
# total_reward = 0

# while not terminated and not truncated:
#     action = np.argmax(q_table[state])
#     new_observation, reward, terminated, truncated, info = env_render.step(action)
#     new_state = discretize_state(new_observation)
#     total_reward += reward
#     state = new_state
#     env_render.render()

# print(f"测试总奖励: {total_reward}")
# env_render.close()
env.close()
```

**注意**: CartPole 环境的观测空间是连续的，上述 Q-Learning 示例对其进行了简单的离散化处理。对于更复杂的连续状态空间环境，通常需要使用深度学习模型（如深度 Q 网络 DQN）来近似 Q 函数，而不是传统的 Q 表。

## 5. 总结与下一步

通过本指南，您应该已经对强化学习的基本概念、Gymnasium 库的使用以及 Q-Learning 算法有了初步的理解。

下一步，您可以尝试：
*   **尝试其他 Gymnasium 环境**: 探索 `Acrobot-v1`, `MountainCar-v0` 等经典控制环境。
*   **深入理解 Q-Learning**: 阅读更多关于其收敛性、超参数调优的资料。
*   **学习深度强化学习 (Deep Reinforcement Learning, DRL)**: 了解 DQN, Policy Gradients (REINFORCE, Actor-Critic), PPO, SAC 等更先进的算法。这些算法通常结合神经网络处理连续或高维状态空间。
*   **参考 Gymnasium 官方文档**: [https://gymnasium.farama.org/](https://gymnasium.farama.org/)

希望这份指南能帮助您开启强化学习的精彩旅程！