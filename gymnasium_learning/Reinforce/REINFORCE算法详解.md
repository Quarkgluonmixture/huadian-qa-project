# 策略梯度 (Policy Gradients) 与 REINFORCE 算法详解

## 1. 策略梯度方法：一种新的强化学习范式

到目前为止，我们学习的 Q-Learning 和 DQN 都属于**基于价值 (Value-based)** 的强化学习方法。它们的核心思想是学习一个价值函数（如 Q 函数），然后根据这个价值函数隐式地推导出最优策略（例如，通过 `argmax` 操作选择 Q 值最大的动作）。

然而，基于价值的方法在某些情况下存在局限性：
1.  **连续动作空间**：当动作空间是连续的时候（例如，机器人关节的扭矩，车辆方向盘的转角），从连续的 Q 值中选择最大动作变得非常困难，通常需要离散化，但这会引入误差和计算复杂度。
2.  **策略复杂性**：有些任务中，最优策略本身可能不是确定性的，而是随机性的。基于价值的方法很难直接学习到这种随机策略。
3.  **局部最优**：当 Q 函数通过神经网络近似时，其优化过程可能导致陷入局部最优。

为了克服这些挑战，**策略梯度 (Policy Gradients)** 方法应运而生。策略梯度方法的核心思想是**直接参数化策略** `π(a|s; θ)`，并**直接优化策略参数 `θ`**，以最大化预期的累积奖励。

### 1.1 策略函数 `π(a|s; θ)`

在策略梯度方法中，我们不再学习价值函数，而是学习一个参数化的策略函数。这个策略函数通常是一个神经网络，输入是状态 `s`，输出是：
*   **离散动作空间**：每个动作 `a` 的概率分布（例如，通过 `softmax` 层输出）。
*   **连续动作空间**：动作的均值和标准差（例如，通过 `tanh` 或 `sigmoid` 激活函数，然后输出均值和标准差来定义一个高斯分布）。

智能体的目标是调整 `θ`，使得智能体能够学习到更好的行为方式。

### 1.2 目标函数：最大化期望回报

策略梯度的目标是最大化智能体在环境中的期望累积奖励。我们定义一个性能指标 `J(θ)`，通常是期望的轨迹回报：

`J(θ) = E_τ [ Σ_{t=0}^T r_t ]`

其中 `τ` 表示一个完整的轨迹（`s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T`），期望是基于策略 `π(a|s; θ)` 和环境动力学 `P(s'|s, a)`。

我们的目标是找到参数 `θ`，使得 `J(θ)` 最大化。这可以通过梯度上升 (Gradient Ascent) 来实现：

`θ ← θ + α ∇_θ J(θ)`

其中 `α` 是学习率，`∇_θ J(θ)` 是策略梯度。

## 2. 策略梯度定理 (Policy Gradient Theorem)

策略梯度定理是策略梯度方法的核心。它提供了一个计算 `∇_θ J(θ)` 的方法，即使我们不知道环境的状态转移概率，这使得策略梯度方法能够应用于无模型（model-free）的强化学习问题。

对于一个有限的马尔可夫决策过程（MDP），策略梯度定理指出：

`∇_θ J(θ) = E_{s~ρ^π, a~π} [ ∇_θ log π(a|s; θ) Q^π(s, a) ]`

其中：
*   `ρ^π(s)` 是策略 `π` 下的稳态分布（state visitation distribution），表示在遵循策略 `π` 的情况下，智能体访问到状态 `s` 的概率。
*   `Q^π(s, a)` 是在遵循策略 `π` 的情况下，在状态 `s` 执行动作 `a` 的动作价值函数。

这个定理告诉我们，我们可以通过采样（在环境中执行策略并收集经验）来估计策略梯度，而不需要知道整个 MDP 的模型。

## 3. REINFORCE 算法 (Monte Carlo Policy Gradient)

REINFORCE (Reward Increment Non-negative Factor × Offset Reinforcement Learning) 算法，也被称为 Monte Carlo Policy Gradient，是策略梯度定理最直接的实现之一。它使用蒙特卡洛（Monte Carlo）方法来估计 `Q^π(s, a)`。

### 3.1 蒙特卡洛估计

在 REINFORCE 中，我们通过一个完整的**回合 (Episode)** 来收集经验。在一个回合结束后，我们计算每个时间步 `t` 的**折扣累积回报 (Discounted Cumulative Return)** `G_t`。`G_t` 是从时间步 `t` 开始，到回合结束时所有未来奖励的折扣和：

`G_t = r_t + γr_{t+1} + γ^2r_{t+2} + ... + γ^{T-t}r_T`

REINFORCE 算法使用这个 `G_t` 来作为 `Q^π(s_t, a_t)` 的无偏估计。

### 3.2 REINFORCE 梯度更新规则

将 `G_t` 代入策略梯度定理，我们得到 REINFORCE 的梯度估计：

`∇_θ J(θ) ≈ Σ_{t=0}^T ∇_θ log π(a_t|s_t; θ) G_t`

在实际实现中，我们通过梯度上升来更新策略参数 `θ`：

`θ ← θ + α Σ_{t=0}^T ∇_θ log π(a_t|s_t; θ) G_t`

或者，更常见的是，对于每个时间步 `t`，我们计算一个损失函数，然后对损失函数进行梯度下降（因为优化器通常是梯度下降）：

`Loss = - Σ_{t=0}^T log π(a_t|s_t; θ) G_t`

然后通过 `optimizer.step()` 更新参数。这里的负号是因为我们要最大化回报，而优化器通常执行最小化操作。

### 3.3 REINFORCE 算法流程

1.  **初始化**：
    *   初始化策略网络 `π(s; θ)`，随机初始化权重 `θ`。
    *   选择优化器（如 Adam）。
    *   设置超参数：学习率 `α`，折扣因子 `γ`，训练回合数 `num_episodes`。

2.  **主循环 (对于每个训练回合 Episode)**：
    *   **收集轨迹**：使用当前策略 `π(s; θ)` 与环境交互，直到回合结束，收集一条完整的轨迹：
        `(s_0, a_0, r_0), (s_1, a_1, r_1), ..., (s_T, a_T, r_T)`。
    *   **计算回报**：对于轨迹中的每个时间步 `t`，计算折扣累积回报 `G_t`。
    *   **计算损失和梯度**：
        *   初始化一个空的损失。
        *   对于轨迹中的每个时间步 `t`：
            *   获取在状态 `s_t` 下采取动作 `a_t` 的对数概率 `log π(a_t|s_t; θ)`。
            *   将损失累加 `Loss += - log π(a_t|s_t; θ) * G_t`。
    *   **更新策略网络**：
        *   清空优化器梯度：`optimizer.zero_grad()`。
        *   反向传播：`Loss.backward()`。
        *   更新参数：`optimizer.step()`。

## 4. REINFORCE 的优缺点

### 4.1 优点

*   **可以直接优化策略**：无需价值函数，可以直接学习到随机策略，更适用于一些本身就是随机最优策略的问题。
*   **适用于连续动作空间**：策略网络可以很容易地输出连续动作的概率分布参数（如均值和方差），从而处理连续动作空间问题。
*   **理论基础扎实**：有策略梯度定理作为坚实的数学基础。

### 4.2 局限性**

*   **高方差 (High Variance)**：REINFORCE 是一个蒙特卡洛方法，它使用整个回合的回报 `G_t` 来估计 `Q^π(s_t, a_t)`。这导致梯度估计的方差很高，使得训练过程不稳定，收敛速度慢。
*   **效率低下**：每个回合只能更新一次策略，并且在更新前需要等待整个回合结束才能计算回报。
*   **不适用于部分可观测环境**：由于依赖完整的轨迹信息，在部分可观测的 MDP (POMDP) 中表现不佳。

### 4.3 降低方差的尝试：基线 (Baseline)

为了降低策略梯度的方差，一个常见的技巧是引入**基线 (Baseline)**。策略梯度定理可以修改为：

`∇_θ J(θ) = E_{s~ρ^π, a~π} [ ∇_θ log π(a|s; θ) (Q^π(s, a) - b(s)) ]`

其中 `b(s)` 是一个基线函数，它只依赖于状态 `s`。基线的引入不会改变梯度的期望值（即不会引入偏差），但可以显著降低方差。一个常见的基线就是状态价值函数 `V^π(s)`。当 `Q^π(s, a) - V^π(s)` 时，这被称为**优势函数 (Advantage Function)** `A^π(s, a)`。

引入基线后，REINFORCE 的梯度更新可以变为：

`θ ← θ + α Σ_{t=0}^T ∇_θ log π(a_t|s_t; θ) (G_t - b(s_t))`

尽管 REINFORCE 简单直观，但其高方差是其主要缺点。这促使了更高级的策略梯度算法，如 Actor-Critic、A2C、A3C、PPO 和 SAC 的发展，它们通过结合价值函数或使用更复杂的技巧来降低方差并提高训练效率。

## 5. 总结

策略梯度方法为强化学习提供了一个强大的替代范式，尤其是在处理连续动作空间和直接学习随机策略时具有优势。REINFORCE 算法作为策略梯度的基础，直观地展示了如何通过蒙特卡洛回报来优化策略。尽管其存在高方差的局限性，但它是理解更高级策略梯度算法（如 Actor-Critic 方法）的基石。掌握 REINFORCE，将帮助您更好地理解现代强化学习算法的演进。
