# 深度Q网络 (DQN) 学习指南

## 简介

恭喜你已经掌握了 Q-learning！现在，让我们进入强化学习的下一个重要里程碑：深度Q网络 (DQN)。

Q-learning 在处理状态数量有限的环境时非常有效。但是，当状态空间变得非常大甚至连续时，使用Q表来存储和更新每个状态-动作对的Q值就变得不可行了。

DQN 通过使用神经网络来近似Q函数，解决了这个问题。我们不再维护一个巨大的Q表，而是训练一个神经网络，输入状态，输出每个动作的Q值。

## DQN 的核心思想

1.  **函数近似**: 使用深度神经网络来近似Q函数：`Q(s, a; θ) ≈ Q*(s, a)`，其中 `θ` 是神经网络的权重。
2.  **经验回放 (Experience Replay)**: 创建一个“回放缓冲区”来存储智能体与环境交互的经历（(s, a, r, s') 元组）。在训练时，我们从缓冲区中随机采样一批经历，而不是使用最近的经历。这打破了数据之间的相关性，使得训练更加稳定。
3.  **目标网络 (Target Network)**: 使用一个独立的、周期性更新的目标网络来计算目标Q值。这有助于缓解Q值更新中的自举（bootstrapping）问题，进一步提高训练的稳定性。

## DQN 与 Q-learning 的主要区别

| 特性 | Q-learning | DQN |
| :--- | :--- | :--- |
| **Q值存储** | Q-表 (表格) | 神经网络 |
| **状态空间** | 有限、离散 | 巨大、连续 |
| **数据使用** | 仅使用当前经验 | 使用经验回放 |
| **更新目标** | 基于下一个状态的Q值 | 基于目标网络的Q值 |

## 下一步

接下来，我们将通过一个实际的代码示例来学习如何实现一个简单的 DQN 智能体，并将其应用于 Gymnasium 中的 `CartPole-v1` 环境。