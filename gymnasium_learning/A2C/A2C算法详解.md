# Actor-Critic (A2C) 算法详解

## 1. Actor-Critic：策略梯度与价值函数的结合

在强化学习中，我们已经学习了两种主要的算法范式：
*   **基于价值 (Value-based)** 的方法（如 Q-Learning, DQN），它们学习一个价值函数来指导决策。
*   **基于策略 (Policy-based)** 的方法（如 REINFORCE），它们直接学习和优化策略。

REINFORCE 算法虽然能够直接优化策略并处理连续动作空间，但其主要的缺点是**高方差**，这导致训练不稳定且收敛速度慢。为了解决这个问题，研究人员提出了 **Actor-Critic (行动者-评论家)** 框架。

Actor-Critic 框架结合了这两种范式的优点：
*   **行动者 (Actor)**: 负责学习策略 `π(a|s; θ)`，并根据策略选择动作。它是一个基于策略的学习者。
*   **评论家 (Critic)**: 负责学习价值函数 `V^π(s; w)` 或 `Q^π(s, a; w)`，评估行动者所采取动作的好坏。它是一个基于价值的学习者。

评论家通过提供一个**基线 (Baseline)** 来帮助行动者降低策略梯度的方差，使得行动者能够更稳定、更有效地学习。

## 2. A2C (Advantage Actor-Critic) 算法核心思想

A2C (Advantage Actor-Critic) 是 Actor-Critic 框架的一个经典实现，它使用**优势函数 (Advantage Function)** 来指导策略更新。

### 2.1 优势函数 `A(s, a)`

回忆策略梯度定理，我们使用 `Q^π(s, a)` 来加权 `log π(a|s; θ)` 的梯度。为了降低方差，我们可以引入一个基线 `b(s)`。当基线为状态价值函数 `V^π(s)` 时，`Q^π(s, a) - V^π(s)` 就得到了优势函数 `A^π(s, a)`。

**优势函数**的定义是：
`A^π(s, a) = Q^π(s, a) - V^π(s)`

优势函数 `A^π(s, a)` 表示在状态 `s` 下采取动作 `a` 比按照策略 `π` 的平均表现好多少。
*   如果 `A^π(s, a) > 0`，说明动作 `a` 比平均水平好，我们应该增加 `log π(a|s; θ)` 的概率。
*   如果 `A^π(s, a) < 0`，说明动作 `a` 比平均水平差，我们应该减少 `log π(a|s; θ)` 的概率。

通过使用优势函数，我们只关注动作相对于平均水平的“优势”，而消除了环境固有奖励的绝对值，从而有效降低了梯度的方差。

### 2.2 如何估计优势函数？

在 A2C 中，我们通常会使用两个神经网络：
1.  **策略网络 (Actor Network)**: 输出动作概率 `π(a|s; θ)`。
2.  **价值网络 (Critic Network)**: 输出状态价值函数 `V(s; w)`。

那么，如何计算 `A(s, a)` 呢？最常见的方式是使用 **TD 误差 (Temporal Difference Error)** 来近似优势函数。

我们知道 `Q^π(s, a)` 的蒙特卡洛估计是 `G_t`（从时间步 `t` 开始的累积回报），而 `V^π(s)` 的 TD(0) 估计是 `r_t + γV^π(s_{t+1})`。

因此，我们可以用 TD 误差来近似优势函数：

`A(s_t, a_t) ≈ r_t + γV(s_{t+1}; w) - V(s_t; w)`

这个 `r_t + γV(s_{t+1}; w)` 项被称为 **TD 目标 (TD Target)**，它代表了对 `Q(s_t, a_t)` 的估计。

### 2.3 策略梯度更新 (Actor)

有了优势函数，行动者的策略梯度更新公式变为：

`∇_θ J(θ) = E_{s~ρ^π, a~π} [ ∇_θ log π(a|s; θ) A^π(s, a) ]`

在实际中，我们同样通过采样经验，并对每个时间步进行梯度上升（或通过最小化负号的损失函数进行梯度下降）：

`Loss_actor = - Σ_{t=0}^T log π(a_t|s_t; θ) * A(s_t, a_t)`

### 2.4 价值函数更新 (Critic)

评论家（价值网络）的任务是准确地估计状态价值函数 `V(s; w)`。这通常通过最小化 TD 误差的均方误差 (MSE) 来实现：

`Loss_critic = (TD_Target - V(s_t; w))^2`

其中 `TD_Target = r_t + γV(s_{t+1}; w)`。评论家网络通过梯度下降来更新其参数 `w`。

## 3. A2C 算法流程 (Synchronous Advantage Actor-Critic)

A2C 算法通常指的是同步更新的 Actor-Critic 方法。它的工作流程如下：

1.  **初始化**：
    *   初始化策略网络 `π(s; θ)` (Actor Network)。
    *   初始化价值网络 `V(s; w)` (Critic Network)。
    *   为两个网络选择优化器（如 Adam）。
    *   设置超参数：学习率 `α_actor`, `α_critic`，折扣因子 `γ`，训练回合数 `num_episodes`。

2.  **主循环 (对于每个训练回合 Episode)**：
    *   **重置环境**：获取初始状态 `s`。
    *   **收集轨迹段**：
        *   使用当前策略 `π(s; θ)` 与环境交互 `N` 步（或直到回合结束）。
        *   收集轨迹段 `(s_0, a_0, r_0, s'_0), ..., (s_{N-1}, a_{N-1}, r_{N-1}, s'_{N-1})`。
        *   **注意**：与 REINFORCE 不同，A2C 可以进行单步或多步更新，而不必等待整个回合结束。这里我们假设一个回合，或者一个小的批次。
    *   **计算优势函数和 TD 目标**：
        *   对于轨迹段中的每个时间步 `t`：
            *   使用价值网络计算 `V(s_t; w)` 和 `V(s_{t+1}; w)`。
            *   计算 TD 目标 `TD_Target_t = r_t + γV(s_{t+1}; w)` (如果 `s_{t+1}` 是终止状态，则 `V(s_{t+1}; w)` 为 0)。
            *   计算优势函数 `Advantage_t = TD_Target_t - V(s_t; w)`。
    *   **更新评论家 (Critic)**：
        *   清空价值网络的梯度：`critic_optimizer.zero_grad()`。
        *   计算价值网络的损失 `Loss_critic = Σ_t (TD_Target_t - V(s_t; w))^2`。
        *   反向传播并更新价值网络参数 `w`：`Loss_critic.backward(); critic_optimizer.step()`。
    *   **更新行动者 (Actor)**：
        *   清空策略网络的梯度：`actor_optimizer.zero_grad()`。
        *   计算策略网络的损失 `Loss_actor = - Σ_t log π(a_t|s_t; θ) * Advantage_t`。
        *   反向传播并更新策略网络参数 `θ`：`Loss_actor.backward(); actor_optimizer.step()`。
    *   **判断回合是否结束**：如果回合结束，则开始新的回合。

## 4. A2C 的优点与局限性

### 4.1 优点

*   **降低方差**：通过引入评论家和优势函数，A2C 能够显著降低策略梯度的方差，使得训练更加稳定。
*   **提高样本效率**：与 REINFORCE 相比，A2C 通常能更快地学习到好的策略，因为它能更有效地利用经验。
*   **适用于连续动作空间**：与 REINFORCE 一样，可以轻松处理连续动作空间。
*   **兼容在线学习**：可以进行单步或多步更新，更适合在线学习场景。

### 4.2 局限性**

*   **偏差引入**：评论家对价值函数的估计可能存在偏差，这会影响策略梯度的准确性。
*   **超参数敏感**：A2C 仍然对学习率、折扣因子、网络结构等超参数敏感，需要仔细调整。
*   **相对复杂**：需要同时训练两个神经网络（Actor 和 Critic），增加了实现和调试的复杂性。
*   **探索问题**：与所有基于策略的方法一样，仍需有效处理探索与利用的平衡，通常使用熵正则化 (Entropy Regularization) 来鼓励探索。

### 4.3 熵正则化 (Entropy Regularization)

为了鼓励策略进行更充分的探索，并防止策略过早地收敛到次优的确定性策略，A2C 算法通常会添加一个**熵损失项**到行动者的损失函数中：

`Loss_actor_total = Loss_actor + β * Entropy_Loss`

其中 `β` 是熵正则化的权重系数，`Entropy_Loss` 是策略的熵。策略的熵越大，表示其随机性越强，探索的程度越高。添加熵损失项有助于防止策略在训练初期变得过于“自信”，从而探索到更多潜在的优化路径。

## 5. 总结与进阶

Actor-Critic 框架是强化学习中一个非常重要且流行的范式，A2C 算法作为其基础实现，成功地结合了策略梯度和价值函数估计的优势。它在降低策略梯度方差方面取得了显著进步，为更复杂的算法（如 Asynchronous Advantage Actor-Critic (A3C)、Proximal Policy Optimization (PPO) 和 Soft Actor-Critic (SAC)）奠定了基础。

掌握 A2C 算法，将使您对策略梯度方法有更深入的理解，并为学习更先进的深度强化学习算法打下坚实的基础。